{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.Download Dataset"
      ],
      "metadata": {
        "id": "2_E11b5vc5Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-m0o70kc_c6",
        "outputId": "e20c079b-c9dc-4228-8d21-cc1e38e99706"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/flowers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvhkG5IsdcCS",
        "outputId": "b1efc39c-ad42-4722-eb15-a5558b843df0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/flowers'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Image Agumentation"
      ],
      "metadata": {
        "id": "LnifPW5adkBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180\n",
        "data_dir = \"/content/drive/MyDrive/flowers\""
      ],
      "metadata": {
        "id": "6X--g9pCdm-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255, horizontal_flip = True, vertical_flip = True, zoom_range = 0.2)\n",
        "x_train = train_datagen.flow_from_directory('/content/drive/MyDrive/flowers',\n",
        "                                                target_size=(64,64),\n",
        "                                                class_mode='categorical',\n",
        "                                                batch_size=100)"
      ],
      "metadata": {
        "id": "2Wx01sgmd6PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 4323 images belonging to 5 classes."
      ],
      "metadata": {
        "id": "EIVaWNBgd7rZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = Sequential(\n",
        "  [\n",
        "    layers.RandomFlip(\"vertical\",input_shape=(img_height, img_width, 3)),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "MgEbKBDreFQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Creating a Model"
      ],
      "metadata": {
        "id": "yhudZiJFePTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Flatten,Dense\n",
        "model = Sequential()\n",
        "training_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=57,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=100)"
      ],
      "metadata": {
        "id": "4ZlQJtq2eSO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 4323 files belonging to 5 classes.\n",
        "Using 3459 files for training."
      ],
      "metadata": {
        "id": "nAW2nhaKecBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_ds = tf.keras.utils.image_dataset_from_directory(data_dir,validation_split=0.2,subset=\"validation\",seed=107,image_size=(img_height, img_width),batch_siz"
      ],
      "metadata": {
        "id": "MwJuVn5Tedpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found 4323 files belonging to 5 classes.\n",
        "Using 864 files for validation."
      ],
      "metadata": {
        "id": "_VpEYw9Leqet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_ds.class_names"
      ],
      "metadata": {
        "id": "jxaiZotjevng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']"
      ],
      "metadata": {
        "id": "oGQnOTize89t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "for data, labels in training_ds.take(1):\n",
        "  for i in range(6):\n",
        "    ax = plt.subplot(2, 3, i + 1)\n",
        "    plt.imshow(data[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(training_ds.class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "udzm8qpbfH_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Add Layers a) Convolution layer\n",
        "\n",
        "b) Maxpooling layer\n",
        "\n",
        "c) Flatten\n",
        "\n",
        "d) Hidden/dense layers\n",
        "\n",
        "e) Output layer"
      ],
      "metadata": {
        "id": "WMt1xrZ3fXx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''a) Convolution layer'''\n",
        "\n",
        "model.add(Convolution2D(32,(3,3),activation=\"relu\",input_shape=(64,64,3)))\n",
        "\n",
        "\n",
        "'''b) Maxpooling layer'''\n",
        "\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "'''c) Flatten'''\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "\n",
        "'''d) Hidden/dense layers'''\n",
        "\n",
        "model.add(Dense(300, activation = \"relu\"))\n",
        "model.add(Dense(150, activation = \"relu\"))\n",
        "\n",
        "\n",
        "'''e) Output layer'''\n",
        "\n",
        "model.add(Dense(5, activation = \"softmax\"))"
      ],
      "metadata": {
        "id": "-oGS2uPEfZ_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Compiling Model"
      ],
      "metadata": {
        "id": "Gxek97Z5fjgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "3muMrIP1fq6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Fit the Model"
      ],
      "metadata": {
        "id": "2KwFDqr7fwLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, epochs = 15, steps_per_epoch = len(x_train))"
      ],
      "metadata": {
        "id": "HEE0ceyqfxPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1/15\n",
        "44/44 [==============================] - 425s 10s/step - loss: 1.6147 - accuracy: 0.3738\n",
        "Epoch 2/15\n",
        "44/44 [==============================] - 36s 811ms/step - loss: 1.1479 - accuracy: 0.5242\n",
        "Epoch 3/15\n",
        "44/44 [==============================] - 37s 836ms/step - loss: 1.0846 - accuracy: 0.5563\n",
        "Epoch 4/15\n",
        "44/44 [==============================] - 37s 841ms/step - loss: 1.0269 - accuracy: 0.6014\n",
        "Epoch 5/15\n",
        "44/44 [==============================] - 40s 897ms/step - loss: 0.9573 - accuracy: 0.6308\n",
        "Epoch 6/15\n",
        "44/44 [==============================] - 38s 865ms/step - loss: 0.9386 - accuracy: 0.6373\n",
        "Epoch 7/15\n",
        "44/44 [==============================] - 38s 858ms/step - loss: 0.8804 - accuracy: 0.6579\n",
        "Epoch 8/15\n",
        "44/44 [==============================] - 37s 842ms/step - loss: 0.8647 - accuracy: 0.6694\n",
        "Epoch 9/15\n",
        "44/44 [==============================] - 38s 855ms/step - loss: 0.8195 - accuracy: 0.6873\n",
        "Epoch 10/15\n",
        "44/44 [==============================] - 37s 836ms/step - loss: 0.8241 - accuracy: 0.6896\n",
        "Epoch 11/15\n",
        "44/44 [==============================] - 37s 845ms/step - loss: 0.8064 - accuracy: 0.6935\n",
        "Epoch 12/15\n",
        "44/44 [==============================] - 37s 842ms/step - loss: 0.7778 - accuracy: 0.6995\n",
        "Epoch 13/15\n",
        "44/44 [==============================] - 37s 833ms/step - loss: 0.7536 - accuracy: 0.7004\n",
        "Epoch 14/15\n",
        "44/44 [==============================] - 37s 821ms/step - loss: 0.7404 - accuracy: 0.7164\n",
        "Epoch 15/15\n",
        "44/44 [==============================] - 37s 831ms/step - loss: 0.7117 - accuracy: 0.7328\n"
      ],
      "metadata": {
        "id": "VUddmnb2f7-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Save The Model"
      ],
      "metadata": {
        "id": "ByK4MtNsgBJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"flowers.h1\")"
      ],
      "metadata": {
        "id": "vw41AGdJgHIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Test the Model"
      ],
      "metadata": {
        "id": "wOfAPw2kgK1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "model = load_model(\"/content/drive/MyDrive/flowers/flowers.h1\")\n",
        "daisy_img = image.load_img('/content/drive/MyDrive/flowers/daisy/100080576_f52e8ee070_n.jpg',target_size=(64,64))\n",
        "x = image.img_to_array(daisy_img)\n",
        "x = np.expand_dims(x,axis=0)\n",
        "predicted_class=model.predict(x)\n",
        "labels = ['daisy','dandelion','roses','sunflowers','tulips']\n",
        "labels[np.argmax(predicted_class)]"
      ],
      "metadata": {
        "id": "lSvqby-ggSZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'sunflowers'"
      ],
      "metadata": {
        "id": "HfiS8g9OgX3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m=daisy_img\n",
        "m"
      ],
      "metadata": {
        "id": "l-G8PByQgcfi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}